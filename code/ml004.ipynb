{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57732c59-3418-47ff-999b-f2fdbe1914d0",
   "metadata": {},
   "source": [
    "This notebook takes a look at the core ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367f0f53-fe3f-4f07-b00d-b517902001d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bda8bcda-f52d-4832-aed5-2f0524e31035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a densely-connected neural net to train.\n",
    "# Each node in the network will belong a layer.\n",
    "# Each input x_ij (layer i, node j) will be the weighted sum of activations from the previous layer, plus a bias term\n",
    "# Its output a_ij (layer i, node j) will be f( x_ij ), where f is an activation function.\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, weights, bias, activation_fn):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.activation_fn = activation_fn\n",
    "        \n",
    "        self.activation = 0 # activation for a given datapoint, calculated in forward pass\n",
    "        self.delta = 0 # delta for back-prop, calculated in backward pass\n",
    "\n",
    "def weighted_sum(weights, activations, bias):\n",
    "    return torch.dot( weights, activations ) + bias;\n",
    "\n",
    "def get_activations(model, layer_index):\n",
    "    activations = [];\n",
    "    for node in model[layer_index -1 ].nodes:\n",
    "        activations.append( node.activation )\n",
    "    return torch.tensor(activations)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, num_weights_per_node, num_nodes, activation_fn):\n",
    "        self.nodes = []\n",
    "        for j in range(num_nodes):\n",
    "            self.nodes.append( Node( torch.rand(num_weights_per_node), torch.rand(1), activation_fn ) )\n",
    "\n",
    "model = [\n",
    "    Layer( 4, 3, \"relu\"),\n",
    "    Layer( 3, 3, \"relu\"),\n",
    "    Layer( 3, 2, \"relu\")\n",
    "]\n",
    "\n",
    "def relu_derivative(a):\n",
    "    return 1.0 if a > 0 else 0.0\n",
    "\n",
    "# Run forward pass on a single data point, data_k\n",
    "def forward_prop_k(model, data_k):\n",
    "    # data_k will be a rank-1 tensor with the same shape as the first layer\n",
    "    for i in range(len(model)):\n",
    "        layer_i = model[i]\n",
    "        activations = get_activations(model, i) if i > 0 else data_k\n",
    "        for j in range(len(layer_i.nodes)):\n",
    "            node_ij = layer_i.nodes[j]\n",
    "            x_ij = weighted_sum(node_ij.weights, activations, node_ij.bias)\n",
    "            a_ij = torch.nn.functional.relu(torch.tensor([x_ij])) # TODO: Use node_ij.activation_fn\n",
    "            node_ij.activation = a_ij\n",
    "\n",
    "# Run backwards propagation for a single data point, data_k\n",
    "def back_prop_k(model, data_k, target_k):\n",
    "\n",
    "    # Start with the output layer\n",
    "    output_nodes = model[ len(model) - 1 ].nodes\n",
    "    for o in range(len(output_nodes)):\n",
    "        output_node = output_nodes[o]\n",
    "        error = target_k[o] - output_node.activation\n",
    "        df_dx = relu_derivative( output_node.activation ) # derivative of activation_fn, evaluated at stored activation\n",
    "        output_node.delta = -2 * error * df_dx\n",
    "\n",
    "    # Then, progress backwards through the layers\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3bb45ebf-ff53-4d97-b5b8-18aebb076b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k = torch.rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "392c0190-c3cc-4da0-b73f-8a2c178b8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_prop_k(model, data_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "250126f2-3e14-49d6-a208-b91485292d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6776])\n",
      "tensor([2.5783])\n",
      "tensor([2.6472])\n"
     ]
    }
   ],
   "source": [
    "for node in model[1].nodes:\n",
    "    print(node.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "00fd68bc-d0e2-4735-88fd-a33c986f9a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "back_prop_k(model, data_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661210c5-1ec1-427b-9a75-34d27440269d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
