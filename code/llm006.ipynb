{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed8cd98-7b94-4a94-87c6-a44ac7d500b5",
   "metadata": {},
   "source": [
    "All the LLM programs I've written so far return a complete response (up to the max_new_tokens setting). This notebook creates a model that streams tokens as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd67531-7694-4458-a32a-3bb3cd58f102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, or twenty-one.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in\n",
      "Took 45.662352311002905 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import time\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tok)\n",
    "t1= time.perf_counter()\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=200)\n",
    "t2= time.perf_counter()\n",
    "print(f\"Took {t2-t1} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f2bf50-b4e9-4d57-b6d8-d15a44db5076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, or twenty-one.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in a word.\n",
      "\n",
      "The number of characters in a word is the number of letters in\n",
      "Took 2.4244050989982497 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import time\n",
    "import torch\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "streamer = TextStreamer(tok)\n",
    "t1= time.perf_counter()\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=200)\n",
    "t2= time.perf_counter()\n",
    "print(f\"Took {t2-t1} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c3cd0b-5a33-429d-bffc-7090ce4bc3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002b6ac19b1f40ae8e23378d4c7be6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import time\n",
    "import torch\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7204113-3c47-4ed8-ab3e-088b3bc62576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Harry Potter was anything but an normal boy.  He was a wizard.  He was a wizard who lived in a cupboard under the stairs.  He was a wizard who lived in a cupboard under the stairs with his aunt and uncle.  He was a wizard who lived in a cupboard under the stairs with his aunt and uncle and his cousin.  He was a wizard who lived in a cupboard under the stairs with his aunt and uncle and his cousin and his aunt’s cat.  He was a wizard who lived in a cupboard under the stairs with his aunt and uncle and his cousin and his aunt’s cat and his aunt’s cat’s kittens.  He was a wizard who lived in a cupboard under the stairs with his aunt and uncle and his cousin and his aunt’s cat and his aunt’s cat’s kittens and his aunt’s cat’s kittens’ kittens.  He was a wizard\n",
      "Took 6.701926237998123 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Harry Potter was anything but an normal boy. \"\n",
    "inputs = tok([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "streamer = TextStreamer(tok)\n",
    "t1= time.perf_counter()\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=200)\n",
    "t2= time.perf_counter()\n",
    "print(f\"Took {t2-t1} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1270c4f-1949-48ab-acca-61e041349dff",
   "metadata": {},
   "source": [
    "Super repetitive. Let's change the generation strategy to beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393187df-a28d-43d4-8d50-faf1c110f7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Harry Potter was anything but an normal boy. "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tok)\n\u001b[1;32m      4\u001b[0m t1\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m----> 5\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m t2\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTook \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt2\u001b[38;5;241m-\u001b[39mt1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds to execute.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1631\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_generation_mode(generation_config, assistant_model)\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streamer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (generation_config\u001b[38;5;241m.\u001b[39mnum_beams \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1632\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1633\u001b[0m     )\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype:\n\u001b[1;32m   1636\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1637\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are calling .generate() with the `input_ids` being on a device type different\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1638\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than your model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms device. `input_ids` is on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, whereas the model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1644\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1."
     ]
    }
   ],
   "source": [
    "prompt = \"Harry Potter was anything but an normal boy. \"\n",
    "inputs = tok([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "streamer = TextStreamer(tok)\n",
    "t1= time.perf_counter()\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=200, num_beams=5)\n",
    "t2= time.perf_counter()\n",
    "print(f\"Took {t2-t1} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51559359-8756-425c-b2c3-0fae546d2108",
   "metadata": {},
   "source": [
    "Oops! We got an error:\n",
    "```\n",
    "ValueError: `streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\n",
    "```\n",
    "\n",
    "So let's not stream the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53caa7a1-034c-4fb4-a192-5a9a630ec280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "Harry Potter was anything but an normal boy.  He lived in a cupboard under the stairs with his aunt, uncle, and cousin, Dudley Dursley.  Dudley was a spoiled brat who was always picking on Harry.  Harry’s parents died in a car crash when he was a baby.  His aunt and uncle didn’t want to take care of him, so they sent him to live with their sister, Petunia, and her husband, Vernon, and their son, Dudley.\n",
      "\n",
      "When Harry was eleven years old, he received a letter from Hogwarts School of Witchcraft and Wizardry.  His aunt and uncle didn’t want him to go to the school, so they didn’t let him open the letter.  But Harry was determined to go to Hogwarts, so he ran away from his aunt and uncle’s house and went to Hogwarts on his own.\n",
      "\n",
      "At Hogwarts\n",
      "\n",
      "Took 8.732398942000145 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Harry Potter was anything but an normal boy. \"\n",
    "inputs = tok([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "streamer = TextStreamer(tok)\n",
    "t1= time.perf_counter()\n",
    "beam_output = model.generate(**inputs, max_new_tokens=200, num_beams=5)\n",
    "t2= time.perf_counter()\n",
    "output=tok.decode(beam_output[0], skip_special_tokens=True)\n",
    "print(f\"Output:\\n{output}\\n\")\n",
    "print(f\"Took {t2-t1} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf2f2ba-d8a6-4560-9b38-d16597eabb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "# Hello world!\n",
      " It all started 10 years ago, when I was 10 years old. I was in the 5th grade, and I wanted to learn how to code. I started with HTML and CSS, and then moved on to JavaScript. I was hooked! I loved the feeling of creating something from scratch, and seeing it come to life on the screen.\n",
      "\n",
      "Over the years, I've continued to learn and grow as a developer. I've worked on a variety of projects, from small personal websites to large-scale enterprise applications. I've also had the opportunity to work with a variety of technologies, including React, Node.js, and Python.\n",
      "\n",
      "Today, I'm a full-stack developer with a passion for building beautiful, user-friendly applications. I'm always looking for new challenges and opportunities to learn and grow. If you're looking for a developer who is passionate about their work and committed to delivering high-quality results, I'\n",
      "\n",
      "Took 8.730228677999548 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"# Hello world!\\n It all started \"\n",
    "inputs = tok([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "t1= time.perf_counter()\n",
    "beam_output = model.generate(**inputs, max_new_tokens=200, num_beams=5)\n",
    "t2= time.perf_counter()\n",
    "output=tok.decode(beam_output[0], skip_special_tokens=True)\n",
    "print(f\"Output:\\n{output}\\n\")\n",
    "print(f\"Took {t2-t1} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab908942-a041-4f11-b24e-2063ebc394e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
