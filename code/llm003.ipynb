{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03dac42c-39da-47c0-9f6e-d4a6f70ef777",
   "metadata": {},
   "source": [
    "This notebook downloads \"mistralai/Mixtral-8x7B\" and then uses it for inference.\n",
    "\n",
    "This model is huge, so this notebook was written on a machine with 8x A100's (a total of 320G of GPU memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f064ed90-2a75-412d-9bf3-ee94ff2a8030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd579183-4427-4923-bd7d-529b6589fd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b909cfb92d411cb14a383273436326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model: took 68.24930608200066 seconds to execute.\n",
      "Creating pipeline: took 0.0003688449996843701 seconds to execute.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B\"\n",
    "\n",
    "t1= time.perf_counter()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "t2= time.perf_counter()\n",
    "print(f\"Loading tokenizer and model: took {t2-t1} seconds to execute.\")\n",
    "# Create a pipeline\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "t3= time.perf_counter()\n",
    "print(f\"Creating pipeline: took {t3-t2} seconds to execute.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a800e328-ad65-4c62-b551-06230f9e0cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========Please type in your prompt=========================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:  The accelerate python library can be used to load very large AI models across multiple GPUs. The way it works is \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on your input.\n",
      "\n",
      "Inferencing using the model: took 26.911936888999662 seconds to execute.\n",
      "The accelerate python library can be used to load very large AI models across multiple GPUs. The way it works is 1) it loads the model into memory, 2) it splits the model into chunks, 3) it loads the chunks into the GPUs, 4) it runs the model on the GPUs, 5) it unloads the chunks from the GPUs, 6) it unloads the model from memory.\n",
      "\n",
      "The problem is that the model is loaded into memory before it is split into chunks. This means that the model must fit into memory before it can be loaded onto the GPUs. This is a problem because the model is too large to fit into memory.\n",
      "\n",
      "The solution is to use the accelerate python library to load the model into memory, split the model into chunks, and load the chunks into the GPUs. This will allow the model to be loaded onto the GPUs without having to fit into memory.\n",
      "\n",
      "## How to use the accelerate python library to load very large AI models across multiple GPUs\n",
      "\n",
      "The accelerate python library is a powerful tool for loading very large AI models across multiple GPUs. It can be used to load models that are too large to fit into\n"
     ]
    }
   ],
   "source": [
    "# Generate code for an input string\n",
    "print(\"\\n=========Please type in your prompt=========================\\n\")\n",
    "user_content = input(\"\\nPrompt: \") # User question\n",
    "user_content.strip()\n",
    "#prompt=f\"[INST] {user_content} [/INST]\"\n",
    "prompt=user_content\n",
    "print(f\"Running inference on your input.\\n\")\n",
    "t1= time.perf_counter()\n",
    "output = pipe(prompt, max_length=256)[0]['generated_text']\n",
    "t2= time.perf_counter()\n",
    "print(f\"Inferencing using the model: took {t2-t1} seconds to execute.\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
